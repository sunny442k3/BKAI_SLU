{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_CoX8Ajymovd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5a6371-e231-4f0e-97cf-a390f44ef911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Collecting huggingface-hub (from accelerate)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: huggingface-hub, accelerate\n",
            "Successfully installed accelerate-0.23.0 huggingface-hub-0.17.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.1.2-py3-none-any.whl (764 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.8/764.8 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.15.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, lightning-utilities, transformers, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 safetensors-0.3.3 tokenizers-0.13.3 torchmetrics-1.1.2 transformers-4.33.2\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install transformers soundfile torchmetrics gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CymJ6TaMBcEM"
      },
      "outputs": [],
      "source": [
        "# !mkdir ./dataset\n",
        "import gdown\n",
        "def drive_download(idx, output):\n",
        "    url = 'https://drive.google.com/uc?id=' + idx\n",
        "    gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEV1_u8wmrGv",
        "outputId": "10b219ea-0250-4a14-bf83-3030cac69d6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZBL3h6bHMmd8MIUNXqg72PucUkC9ZSWJ\n",
            "To: /content/dataset/train_data.zip\n",
            "100%|██████████| 733M/733M [00:06<00:00, 106MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZepptsTrVSjQEx-dpBBmQ2b7xYFLn_64\n",
            "To: /content/dataset/public_test.zip\n",
            "100%|██████████| 131M/131M [00:01<00:00, 72.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Data download\n",
        "drive_download(\"1ZBL3h6bHMmd8MIUNXqg72PucUkC9ZSWJ\", \"./dataset/train_data.zip\")\n",
        "drive_download(\"1ZepptsTrVSjQEx-dpBBmQ2b7xYFLn_64\", \"./dataset/public_test.zip\")\n",
        "# drive_download(\"1K_07kix1OgBGO2FNPh-Lxqr1yLbtqFYt\", \"./dataset/train.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJZDIHlnBeFT"
      },
      "outputs": [],
      "source": [
        "!unzip ./dataset/public_test.zip -d ./dataset/test\n",
        "!unzip ./dataset/train_data.zip -d ./dataset/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLOT8SkPm3Gk",
        "outputId": "bebff642-4f68-447e-9207-2376831b32ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=186Tv-dPED5QiIJy4sRvlsNsvYxLpXfWX\n",
            "To: /content/vn_base_vocab.json\n",
            "100%|██████████| 1.35k/1.35k [00:00<00:00, 5.00MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KXn1A17ce7jNX1qQhx8l0DQK1KYhbmls\n",
            "To: /content/train_token_labels_20230909.json\n",
            "100%|██████████| 271k/271k [00:00<00:00, 3.62MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K-oNwBu2svshAkmifU9wISKPMvKgeKy4\n",
            "To: /content/train_20230909.jsonl\n",
            "100%|██████████| 3.30M/3.30M [00:00<00:00, 21.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Data vocab, sentence, and token label\n",
        "# drive_download(\"186Tv-dPED5QiIJy4sRvlsNsvYxLpXfWX\", \"./vn_base_vocab.json\")\n",
        "# drive_download(\"1KXn1A17ce7jNX1qQhx8l0DQK1KYhbmls\", \"./train_token_labels_20230909.json\")\n",
        "# drive_download(\"1K-oNwBu2svshAkmifU9wISKPMvKgeKy4\", \"./train_20230909.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDoosxEknlZu"
      },
      "outputs": [],
      "source": [
        "#  Upload py file: model.py, utils.py, dataset.py, trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tpqlqXOGnCLU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import utils\n",
        "from trainer import Trainer, TrainerV2\n",
        "from model import BertSLU, BertSLUV2\n",
        "from functools import partial\n",
        "from dataset import BertDataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eN73cW1enEKT"
      },
      "outputs": [],
      "source": [
        "def custom_collate(tokenizer, is_train, batch):\n",
        "    inputs = tokenizer([i[\"text\"] for i in batch], return_tensors=\"pt\", padding=\"longest\")\n",
        "    if not is_train:\n",
        "        return inputs, torch.zeros_like(inputs[\"input_ids\"]), torch.zeros(inputs[\"input_ids\"].size(0))\n",
        "    seq_len = inputs[\"input_ids\"].size(1)\n",
        "    token_labels = torch.stack([\n",
        "        torch.tensor(i[\"token_label\"] + [-100]*(seq_len - len(i[\"token_label\"]))) for i in batch\n",
        "    ])\n",
        "    intent_labels = torch.tensor([i[\"intent_label\"] for i in batch])\n",
        "    return inputs, token_labels, intent_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6mVloO5enEMw"
      },
      "outputs": [],
      "source": [
        "def get_loader(annotation_path, token_label_path, batch_size=2, test_size=0.3):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "    all_data = utils.load_json(token_label_path)\n",
        "    all_data = [v for k, v in all_data.items()]\n",
        "    all_text = [i[\"sentence\"] for i in all_data]\n",
        "    dataset = BertDataset(all_text, all_data)\n",
        "    N = len(dataset)\n",
        "    print(\"Len dataset\", N)\n",
        "    train_size = int(N * (1-test_size))\n",
        "    train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, N-train_size])\n",
        "    if test_size == 0:\n",
        "        train_set = dataset\n",
        "        valid_set = dataset\n",
        "    train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=partial(custom_collate, tokenizer, True)\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_set,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=partial(custom_collate, tokenizer, True)\n",
        "    )\n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a7nbfvJU63f5"
      },
      "outputs": [],
      "source": [
        "def get_test_loader(test_path, batch_size=2):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "    sequences = utils.load_json(test_path)\n",
        "    id_seqs = [k for k, v in sequences.items()]\n",
        "    seqs = [v for k, v in sequences.items()]\n",
        "    dataset = BertDataset(seqs)\n",
        "    test_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=partial(custom_collate, tokenizer, False)\n",
        "    )\n",
        "    return test_loader, id_seqs, seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ4DbGdjnEPR",
        "outputId": "36c5cc68-8430-438e-c5a2-823d0827de54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len dataset 7490\n",
            "Len train_loader: 164 - Len valid_loader: 71\n"
          ]
        }
      ],
      "source": [
        "train_loader, valid_loader = get_loader(\"./train_20230909.jsonl\", \"./train_token_labels_20230909.json\", 32)\n",
        "print(f\"Len train_loader: {len(train_loader)} - Len valid_loader: {len(valid_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbAISnQTBqEW",
        "outputId": "fd45862f-06d7-48ca-a639-bcc4865073f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test_loader, test_file_id, all_seqs = get_test_loader(\"./4gram_test_sentences_v3_32w.json\", 32)\n",
        "len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./checkpoint"
      ],
      "metadata": {
        "id": "BkhQPD6IXqd6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ./checkpoint/slu_intent.pt ./checkpoint/slu_token.pt"
      ],
      "metadata": {
        "id": "SLsELIN1K8SD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PfOxuR5nnEHy"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "    epochs = 15\n",
        "    checkpoint_path = \"./checkpoint/slu_intent.pt\"\n",
        "    learning_rate = 5e-5\n",
        "    adam_eps = 1e-8\n",
        "    warmup_steps = 1000\n",
        "    weight_decay = 0.005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrz_jhkRnERs",
        "outputId": "05faf76a-626f-4085-879c-95e41ca0c6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of param: 135009807\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "intent_model = BertSLUV2(\"intent_class\", 15, 9, \"vinai/phobert-base-v2\")\n",
        "print(f\"Num of param:\", sum(p.numel() for p in intent_model.parameters()))\n",
        "optimizer = torch.optim.AdamW(intent_model.parameters(), lr=config.learning_rate, eps=config.adam_eps, weight_decay=config.weight_decay)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-knZKV8BnEUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111c6c92-58a9-4189-f606-24d1601f0a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: Tesla T4\n",
            "Total update step: 2460\n",
            "Epoch: 1\n",
            "Train step: 164 / 164 - loss: 0.16437 - acc: 0.9630\n",
            "Valid step: 71 / 71 - loss: 0.00003 - acc: 1.0000\n",
            "\t => Train loss: 0.65861 - Train acc: 0.85\n",
            "\t => Valid loss: 0.07309 - Valid acc: 0.99\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 2\n",
            "Train step: 164 / 164 - loss: 0.00331 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.07080 - Train acc: 0.99\n",
            "\t => Valid loss: 0.04030 - Valid acc: 0.99\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 3\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.04880 - Train acc: 0.99\n",
            "\t => Valid loss: 0.05464 - Valid acc: 0.99\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 4\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.01447 - Train acc: 1.00\n",
            "\t => Valid loss: 0.04199 - Valid acc: 1.00\n",
            "\t => Time: 0:00:31/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 5\n",
            "Train step: 164 / 164 - loss: 0.14251 - acc: 0.9630\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.01726 - Train acc: 1.00\n",
            "\t => Valid loss: 0.10038 - Valid acc: 0.99\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 6\n",
            "Train step: 164 / 164 - loss: 0.17636 - acc: 0.9630\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.02759 - Train acc: 0.99\n",
            "\t => Valid loss: 0.06515 - Valid acc: 0.99\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 7\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.02321 - Train acc: 1.00\n",
            "\t => Valid loss: 0.02753 - Valid acc: 1.00\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 8\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.01231 - Train acc: 1.00\n",
            "\t => Valid loss: 0.04537 - Valid acc: 1.00\n",
            "\t => Time: 0:00:31/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 9\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.00378 - Train acc: 1.00\n",
            "\t => Valid loss: 0.07381 - Valid acc: 0.99\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 10\n",
            "Train step: 164 / 164 - loss: 0.00005 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.00539 - Train acc: 1.00\n",
            "\t => Valid loss: 0.11343 - Valid acc: 0.99\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 11\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.19359 - Train acc: 0.97\n",
            "\t => Valid loss: 0.05731 - Valid acc: 0.99\n",
            "\t => Time: 0:00:31/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 12\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.02410 - Train acc: 1.00\n",
            "\t => Valid loss: 0.03516 - Valid acc: 1.00\n",
            "\t => Time: 0:00:31/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 13\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.00451 - Train acc: 1.00\n",
            "\t => Valid loss: 0.07389 - Valid acc: 0.99\n",
            "\t => Time: 0:00:31/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 14\n",
            "Train step: 164 / 164 - loss: 0.01877 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.00894 - Train acc: 1.00\n",
            "\t => Valid loss: 0.07570 - Valid acc: 0.99\n",
            "\t => Time: 0:00:30/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 15\n",
            "Train step: 164 / 164 - loss: 0.00000 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00000 - acc: 1.0000\n",
            "\t => Train loss: 0.01539 - Train acc: 1.00\n",
            "\t => Valid loss: 0.12193 - Valid acc: 0.99\n",
            "\t => Time: 0:00:31/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n"
          ]
        }
      ],
      "source": [
        "trainer = TrainerV2(intent_model, optimizer, criterion, amp=False, device=device)\n",
        "trainer.fit(train_loader, valid_loader, epochs=config.epochs, checkpoint=config.checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "token_model = BertSLUV2(\"token_class\", 15, 9, \"vinai/phobert-base-v2\")\n",
        "print(f\"Num of param:\", sum(p.numel() for p in token_model.parameters()))\n",
        "optimizer = torch.optim.AdamW(token_model.parameters(), lr=config.learning_rate, eps=config.adam_eps, weight_decay=config.weight_decay)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g3exhAyHt81",
        "outputId": "c35dbc17-6c49-410e-94b6-2254e6cd5b11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of param: 135005193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainerV2(token_model, optimizer, criterion, amp=False, device=device)\n",
        "trainer.fit(train_loader, valid_loader, epochs=config.epochs, checkpoint=config.checkpoint_path)"
      ],
      "metadata": {
        "id": "Mjmhu2mAHxKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnTktstI_m5"
      },
      "source": [
        "# **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubTk8891gdeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f5e409-4da0-4782-a190-5cbb2ffa57e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZBpjMakFSDdShWbn1_BHstr--wRCnlv7\n",
            "To: /content/checkpoint_bert.pt\n",
            "100%|██████████| 540M/540M [00:06<00:00, 89.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "drive_download(\"1ZBpjMakFSDdShWbn1_BHstr--wRCnlv7\", \"./checkpoint_bert.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUiLWtzAgdri"
      },
      "outputs": [],
      "source": [
        "!cp -r ./drive/MyDrive/datasets/checkpoint_bert.pt ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model = BertSLUV2(\"intent_class\", 15, 9, \"vinai/phobert-base-v2\")\n",
        "token_model = BertSLUV2(\"token_class\", 15, 9, \"vinai/phobert-base-v2\")\n",
        "intent_trainer = TrainerV2(intent_model, optimizer, criterion, amp=False, device=device)\n",
        "intent_trainer.load_checkpoint(\"./checkpoint/slu_intent.pt\")\n",
        "token_trainer = TrainerV2(token_model, optimizer, criterion, amp=False, device=device)\n",
        "token_trainer.load_checkpoint(\"./checkpoint/slu_token.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b70jBd_zKrNr",
        "outputId": "77f1fc11-1b13-4ea1-ef8d-bd4c52955c61"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Model load successful\n",
            "[+] Model load successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEhRLDy7Cnhi",
        "outputId": "b233d797-0143-41db-fed1-07dffbaa8b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 41 / 41\n",
            " 41 / 41\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1299, 1299)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# trainer = TrainerV2(model, optimizer, criterion, amp=False, device=device)\n",
        "# intent_trainer.load_checkpoint(\"./checkpoint/checkpoint_bert.pt\")\n",
        "_ , all_intents = intent_trainer.test(test_loader)\n",
        "all_tokens, _ = token_trainer.test(test_loader)\n",
        "len(all_tokens), len(all_intents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW5JCiDBFWsA",
        "outputId": "cabe0415-e92b-4055-87cb-6cb19636c08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_intents[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeZ-AWWzNgF9",
        "outputId": "7626a07d-f926-49c2-ea87-3684e5338465"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 0, 14, 3, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "92ZSv_lXJR0l"
      },
      "outputs": [],
      "source": [
        "INVERSE_MAP_TOKENS = {\n",
        "    0: 'word',\n",
        "    1: 'time at',\n",
        "    2: 'device',\n",
        "    3: 'changing value',\n",
        "    4: 'scene',\n",
        "    5: 'command',\n",
        "    6: 'location',\n",
        "    7: 'duration',\n",
        "    8: 'target number'\n",
        " }\n",
        "\n",
        "INVERSE_MAP_INTENTS = {\n",
        "    0: 'Giảm độ sáng của thiết bị',\n",
        "    1: 'Đóng thiết bị',\n",
        "    2: 'Hủy hoạt cảnh',\n",
        "    3: 'Tắt thiết bị',\n",
        "    4: 'Tăng âm lượng của thiết bị',\n",
        "    5: 'Giảm mức độ của thiết bị',\n",
        "    6: 'Bật thiết bị',\n",
        "    7: 'Tăng mức độ của thiết bị',\n",
        "    8: 'Tăng nhiệt độ của thiết bị',\n",
        "    9: 'Kiểm tra tình trạng thiết bị',\n",
        "    10: 'Mở thiết bị',\n",
        "    11: 'Giảm âm lượng của thiết bị',\n",
        "    12: 'Kích hoạt cảnh',\n",
        "    13: 'Giảm nhiệt độ của thiết bị',\n",
        "    14: 'Tăng độ sáng của thiết bị'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "p-QhP_ztFwzn"
      },
      "outputs": [],
      "source": [
        "def collect_label(token):\n",
        "    token = token[1:]\n",
        "    for i in range(len(token) - 1, -1, -1):\n",
        "        if token[i] != 0:\n",
        "            token = token[:i+1]\n",
        "            break\n",
        "    token += [-1]\n",
        "    map_labels = []\n",
        "    cur = 0\n",
        "    val = token[0]\n",
        "    for idx, i in enumerate(token[1:], 1):\n",
        "        if i == val:\n",
        "            continue\n",
        "        else:\n",
        "            if val != 0:\n",
        "                map_labels.append([cur, idx-1, val])\n",
        "            val = i\n",
        "            cur = idx\n",
        "    return map_labels\n",
        "\n",
        "def convert_into_output(all_tokens, all_intents, all_seqs, test_file_id, tokenizer):\n",
        "    ans = []\n",
        "    for idx in range(len(all_tokens)):\n",
        "        token = all_tokens[idx]\n",
        "        intent = all_intents[idx]\n",
        "        seq = tokenizer.tokenize(all_seqs[idx])\n",
        "        labels = collect_label(token)\n",
        "        tmp_ans = {\n",
        "            \"intent\": INVERSE_MAP_INTENTS[intent],\n",
        "            \"file\": test_file_id[idx]\n",
        "        }\n",
        "        entities = []\n",
        "        # print(labels)\n",
        "        # print(seq)\n",
        "        # return\n",
        "        for label in labels:\n",
        "            if label[-1] == 0:\n",
        "                continue\n",
        "            sub_text = seq[label[0]: label[1]+1]\n",
        "            sub_text = tokenizer.decode(\n",
        "                tokenizer.convert_tokens_to_ids(sub_text), skip_special_tokens=True\n",
        "            )\n",
        "            tmp_add = {\"type\": INVERSE_MAP_TOKENS[label[-1]], \"filler\": sub_text}\n",
        "            # check = list(filter(lambda x: tmp_add[\"type\"] == x[\"type\"] and tmp_add[\"filler\"] == x[\"filler\"], entities))\n",
        "            # if len(check):\n",
        "                # continue\n",
        "            entities += [tmp_add]\n",
        "        tmp_ans[\"entities\"] = entities\n",
        "        ans.append(tmp_ans)\n",
        "        print(\"\\r\", end=\"\")\n",
        "        print(f\"\\r {idx+1} / {len(all_tokens)}\", end=\"\")\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_COeQeYzINjD",
        "outputId": "09038c3f-686a-4af8-a582-81df1a77d827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1299 / 1299"
          ]
        }
      ],
      "source": [
        "ans = convert_into_output(all_tokens, all_intents, all_seqs, test_file_id, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fBarcgXLXnr",
        "outputId": "955bbbf3-fa43-4c80-c375-cf7908733f62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'intent': 'Bật thiết bị',\n",
              "  'file': 'qPANF1Bx3XpmuIEjlPUm9Ez.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'bật'},\n",
              "   {'type': 'time at', 'filler': '9 giờ 40 phút'}]},\n",
              " {'intent': 'Giảm độ sáng của thiết bị',\n",
              "  'file': '8LcLj1sHy9xAZF4ibvlPFca.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'giảm'},\n",
              "   {'type': 'changing value', 'filler': '33%'}]},\n",
              " {'intent': 'Tăng độ sáng của thiết bị',\n",
              "  'file': 'Z5G73Vc0YuNWlgV48QZYyQD.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'tăng'},\n",
              "   {'type': 'changing value', 'filler': '88%'}]},\n",
              " {'intent': 'Tắt thiết bị',\n",
              "  'file': 'jTA7bLvVi4zxyXt8c3ePOTT.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'tắt'},\n",
              "   {'type': 'device', 'filler': 'bóng sân'}]},\n",
              " {'intent': 'Đóng thiết bị',\n",
              "  'file': 'QaiOJwzIYKRxVrLDQHxODfn.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'đóng'},\n",
              "   {'type': 'device', 'filler': 'lò nướng'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "ans[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HvN-RjVNL_T7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"./predictions.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in ans:\n",
        "        json.dump(line, f)\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWsB4vdu8Op6",
        "outputId": "94d228dc-f296-4259-c27d-1f4a85a20b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBMr_GrpEVpH"
      },
      "outputs": [],
      "source": [
        "!cp -r ./checkpoint/checkpoint_bert.pt ./drive/MyDrive/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N-3adxegs7n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}