{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CoX8Ajymovd"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install transformers soundfile torchmetrics gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDoosxEknlZu"
      },
      "outputs": [],
      "source": [
        "#  Upload py file: model.py, utils.py, dataset.py, trainer.py, train_token_labels.json for training, test_sentences.json for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tpqlqXOGnCLU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import utils\n",
        "from trainer import Trainer, TrainerV2\n",
        "from model import BertSLU, BertSLUV2, BertSLUV3\n",
        "from functools import partial\n",
        "from dataset import BertDataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eN73cW1enEKT"
      },
      "outputs": [],
      "source": [
        "def custom_collate(tokenizer, is_train, batch):\n",
        "    inputs = tokenizer([i[\"text\"] for i in batch], return_tensors=\"pt\", padding=\"longest\")\n",
        "    if not is_train:\n",
        "        return inputs, torch.zeros_like(inputs[\"input_ids\"]), torch.zeros(inputs[\"input_ids\"].size(0))\n",
        "    seq_len = inputs[\"input_ids\"].size(1)\n",
        "    token_labels = torch.stack([\n",
        "        torch.tensor(i[\"token_label\"] + [-100]*(seq_len - len(i[\"token_label\"]))) for i in batch\n",
        "    ])\n",
        "    intent_labels = torch.tensor([i[\"intent_label\"] for i in batch])\n",
        "    return inputs, token_labels, intent_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6mVloO5enEMw"
      },
      "outputs": [],
      "source": [
        "def get_loader(annotation_path, token_label_path, batch_size=2, test_size=0.3):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "    all_data = utils.load_json(token_label_path)\n",
        "    all_data = [v for k, v in all_data.items()]\n",
        "    all_text = [i[\"sentence\"] for i in all_data]\n",
        "    dataset = BertDataset(all_text, all_data)\n",
        "    N = len(dataset)\n",
        "    print(\"Len dataset\", N)\n",
        "    train_size = int(N * (1-test_size))\n",
        "    train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, N-train_size])\n",
        "    if test_size == 0:\n",
        "        train_set = dataset\n",
        "        valid_set = dataset\n",
        "    train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=partial(custom_collate, tokenizer, True)\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_set,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=partial(custom_collate, tokenizer, True)\n",
        "    )\n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "a7nbfvJU63f5"
      },
      "outputs": [],
      "source": [
        "def get_test_loader(test_path, batch_size=2):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "    sequences = utils.load_json(test_path)\n",
        "    id_seqs = [k for k, v in sequences.items()]\n",
        "    seqs = [v for k, v in sequences.items()]\n",
        "    dataset = BertDataset(seqs)\n",
        "    test_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=partial(custom_collate, tokenizer, False)\n",
        "    )\n",
        "    return test_loader, id_seqs, seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ4DbGdjnEPR",
        "outputId": "92bcd93d-ffcd-4fd2-fe52-c74364070f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len dataset 7490\n",
            "Len train_loader: 164 - Len valid_loader: 71\n"
          ]
        }
      ],
      "source": [
        "train_loader, valid_loader = get_loader(\"./train_20230909.jsonl\", \"./train_token_labels_20230909.json\", 32)\n",
        "print(f\"Len train_loader: {len(train_loader)} - Len valid_loader: {len(valid_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbAISnQTBqEW",
        "outputId": "fefa94e4-8f22-4cb2-a376-5edc30c46556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "test_loader, test_file_id, all_seqs = get_test_loader(\"./4gram_test_sentences_v3_32w.json\", 32)\n",
        "len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./checkpoint"
      ],
      "metadata": {
        "id": "BkhQPD6IXqd6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PfOxuR5nnEHy"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "    epochs = 15\n",
        "    checkpoint_path_it = \"./checkpoint/slu_intent.pt\"\n",
        "    checkpoint_path_tk = \"./checkpoint/slu_token.pt\"\n",
        "    learning_rate = 1e-5\n",
        "    adam_eps = 1e-8\n",
        "    warmup_steps = 1000\n",
        "    weight_decay = 0.005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrz_jhkRnERs",
        "outputId": "8add59a0-fcbb-42e5-fd19-ad2a24f29d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of param: 157673999\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "intent_model = BertSLUV3(\"intent_class\", 15, 9, \"vinai/phobert-base-v2\")\n",
        "print(f\"Num of param:\", sum(p.numel() for p in intent_model.parameters()))\n",
        "optimizer = torch.optim.AdamW(intent_model.parameters(), lr=config.learning_rate, eps=config.adam_eps, weight_decay=config.weight_decay)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-knZKV8BnEUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65b92ccc-0dce-4a3b-f0e8-c915bed199a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Model load successful\n",
            "Running on: Tesla T4\n",
            "Total update step: 2460\n",
            "Epoch: 1\n",
            "Train step: 164 / 164 - loss: 0.00052 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00005 - acc: 1.0000\n",
            "\t => Train loss: 0.00340 - Train acc: 1.00\n",
            "\t => Valid loss: 0.04433 - Valid acc: 1.00\n",
            "\t => Time: 0:00:39/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 2\n",
            "Train step: 164 / 164 - loss: 0.00001 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00003 - acc: 1.0000\n",
            "\t => Train loss: 0.01269 - Train acc: 1.00\n",
            "\t => Valid loss: 0.04624 - Valid acc: 1.00\n",
            "\t => Time: 0:00:38/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 3\n",
            "Train step: 164 / 164 - loss: 0.00001 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00001 - acc: 1.0000\n",
            "\t => Train loss: 0.00006 - Train acc: 1.00\n",
            "\t => Valid loss: 0.06051 - Valid acc: 0.99\n",
            "\t => Time: 0:00:39/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 4\n",
            "Train step: 164 / 164 - loss: 0.00624 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00145 - acc: 1.0000\n",
            "\t => Train loss: 0.02497 - Train acc: 1.00\n",
            "\t => Valid loss: 0.13155 - Valid acc: 0.98\n",
            "\t => Time: 0:00:38/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 5\n",
            "Train step: 164 / 164 - loss: 0.00006 - acc: 1.0000\n",
            "Valid step: 71 / 71 - loss: 0.00001 - acc: 1.0000\n",
            "\t => Train loss: 0.02805 - Train acc: 1.00\n",
            "\t => Valid loss: 0.07745 - Valid acc: 0.99\n",
            "\t => Time: 0:00:38/step - lr: : 5.000000e-05\n",
            "[+] Save checkpoint successfully\n",
            "Epoch: 6\n",
            "Train step: 69 / 164 - loss: 0.00001 - acc: 1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTrain step: 70 / 164 - loss: 0.00003 - acc: 1.0000Traceback (most recent call last):\n",
            "  File \"/content/trainer.py\", line 302, in fit\n",
            "    self.forward(train_loader, \"train\")\n",
            "  File \"/content/trainer.py\", line 276, in forward\n",
            "    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\", line 76, in clip_grad_norm_\n",
            "    torch._foreach_mul_(grads, clip_coef_clamped.to(device))  # type: ignore[call-overload]\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-23-3dde86392a67>\", line 3, in <cell line: 3>\n",
            "    trainer.fit(train_loader, valid_loader, epochs=config.epochs, checkpoint=config.checkpoint_path)\n",
            "  File \"/content/trainer.py\", line 307, in fit\n",
            "    sys.exit()\n",
            "SystemExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, epochs, checkpoint)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/trainer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dataloader, fw_mode)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforeach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_has_foreach_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3dde86392a67>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./checkpoint/slu_intent.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, epochs, checkpoint)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "trainer = TrainerV2(intent_model, optimizer, criterion, amp=False, device=device)\n",
        "trainer.load_checkpoint(\"./checkpoint/slu_intent.pt\")\n",
        "trainer.fit(train_loader, valid_loader, epochs=config.epochs, checkpoint=config.checkpoint_path_it)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "token_model = BertSLUV3(\"token_class\", 15, 9, \"vinai/phobert-base-v2\")\n",
        "print(f\"Num of param:\", sum(p.numel() for p in token_model.parameters()))\n",
        "optimizer = torch.optim.AdamW(token_model.parameters(), lr=config.learning_rate, eps=config.adam_eps, weight_decay=config.weight_decay)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g3exhAyHt81",
        "outputId": "e661a953-e5ee-4811-ed0d-6cd293ffd96f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of param: 135005193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainerV2(token_model, optimizer, criterion, amp=False, device=device)\n",
        "trainer.fit(train_loader, valid_loader, epochs=config.epochs, checkpoint=config.checkpoint_path_tk)"
      ],
      "metadata": {
        "id": "Mjmhu2mAHxKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnTktstI_m5"
      },
      "source": [
        "# **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_N-3adxegs7n"
      },
      "outputs": [],
      "source": [
        "!cp -r ./drive/MyDrive/checkpoint/slu_token.pt ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model = BertSLUV3(\"intent_class\", 15, 9, \"vinai/phobert-base-v2\")\n",
        "token_model = BertSLUV3(\"token_class\", 15, 9, \"vinai/phobert-base-v2\")\n",
        "intent_trainer = TrainerV2(intent_model, optimizer, criterion, amp=False, device=device)\n",
        "intent_trainer.load_checkpoint(\"./checkpoint/slu_intent.pt\")\n",
        "token_trainer = TrainerV2(token_model, optimizer, criterion, amp=False, device=device)\n",
        "token_trainer.load_checkpoint(\"./slu_token.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b70jBd_zKrNr",
        "outputId": "d24d6e42-9a87-4d8a-d40c-c2c6ee8d7f2f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Model load successful\n",
            "[+] Model load successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEhRLDy7Cnhi",
        "outputId": "d48807a9-7fdc-46f9-9e94-f1708a19bf9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 41 / 41\n",
            " 41 / 41\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1299, 1299)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# trainer = TrainerV2(model, optimizer, criterion, amp=False, device=device)\n",
        "# intent_trainer.load_checkpoint(\"./checkpoint/checkpoint_bert.pt\")\n",
        "_ , all_intents = intent_trainer.test(test_loader)\n",
        "all_tokens, _ = token_trainer.test(test_loader)\n",
        "len(all_tokens), len(all_intents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW5JCiDBFWsA",
        "outputId": "a8d6fb3f-bdf7-48c3-9337-21e5dc43548d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_intents[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeZ-AWWzNgF9",
        "outputId": "7626a07d-f926-49c2-ea87-3684e5338465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 0, 14, 3, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "92ZSv_lXJR0l"
      },
      "outputs": [],
      "source": [
        "INVERSE_MAP_TOKENS = {\n",
        "    0: 'word',\n",
        "    1: 'time at',\n",
        "    2: 'device',\n",
        "    3: 'changing value',\n",
        "    4: 'scene',\n",
        "    5: 'command',\n",
        "    6: 'location',\n",
        "    7: 'duration',\n",
        "    8: 'target number'\n",
        " }\n",
        "\n",
        "INVERSE_MAP_INTENTS = {\n",
        "    0: 'Giảm độ sáng của thiết bị',\n",
        "    1: 'Đóng thiết bị',\n",
        "    2: 'Hủy hoạt cảnh',\n",
        "    3: 'Tắt thiết bị',\n",
        "    4: 'Tăng âm lượng của thiết bị',\n",
        "    5: 'Giảm mức độ của thiết bị',\n",
        "    6: 'Bật thiết bị',\n",
        "    7: 'Tăng mức độ của thiết bị',\n",
        "    8: 'Tăng nhiệt độ của thiết bị',\n",
        "    9: 'Kiểm tra tình trạng thiết bị',\n",
        "    10: 'Mở thiết bị',\n",
        "    11: 'Giảm âm lượng của thiết bị',\n",
        "    12: 'Kích hoạt cảnh',\n",
        "    13: 'Giảm nhiệt độ của thiết bị',\n",
        "    14: 'Tăng độ sáng của thiết bị'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "p-QhP_ztFwzn"
      },
      "outputs": [],
      "source": [
        "def collect_label(token):\n",
        "    token = token[1:]\n",
        "    for i in range(len(token) - 1, -1, -1):\n",
        "        if token[i] != 0:\n",
        "            token = token[:i+1]\n",
        "            break\n",
        "    token += [-1]\n",
        "    map_labels = []\n",
        "    cur = 0\n",
        "    val = token[0]\n",
        "    for idx, i in enumerate(token[1:], 1):\n",
        "        if i == val:\n",
        "            continue\n",
        "        else:\n",
        "            if val != 0:\n",
        "                map_labels.append([cur, idx-1, val])\n",
        "            val = i\n",
        "            cur = idx\n",
        "    return map_labels\n",
        "\n",
        "def convert_into_output(all_tokens, all_intents, all_seqs, test_file_id, tokenizer):\n",
        "    ans = []\n",
        "    for idx in range(len(all_tokens)):\n",
        "        token = all_tokens[idx]\n",
        "        intent = all_intents[idx]\n",
        "        seq = tokenizer.tokenize(all_seqs[idx])\n",
        "        labels = collect_label(token)\n",
        "        tmp_ans = {\n",
        "            \"intent\": INVERSE_MAP_INTENTS[intent],\n",
        "            \"file\": test_file_id[idx]\n",
        "        }\n",
        "        entities = []\n",
        "        # print(labels)\n",
        "        # print(seq)\n",
        "        # return\n",
        "        for label in labels:\n",
        "            if label[-1] == 0:\n",
        "                continue\n",
        "            sub_text = seq[label[0]: label[1]+1]\n",
        "            sub_text = tokenizer.decode(\n",
        "                tokenizer.convert_tokens_to_ids(sub_text), skip_special_tokens=True\n",
        "            )\n",
        "            tmp_add = {\"type\": INVERSE_MAP_TOKENS[label[-1]], \"filler\": sub_text}\n",
        "            # check = list(filter(lambda x: tmp_add[\"type\"] == x[\"type\"] and tmp_add[\"filler\"] == x[\"filler\"], entities))\n",
        "            # if len(check):\n",
        "                # continue\n",
        "            entities += [tmp_add]\n",
        "        tmp_ans[\"entities\"] = entities\n",
        "        ans.append(tmp_ans)\n",
        "        print(\"\\r\", end=\"\")\n",
        "        print(f\"\\r {idx+1} / {len(all_tokens)}\", end=\"\")\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_COeQeYzINjD",
        "outputId": "c0148d44-580b-4390-bdc1-94581709c181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1299 / 1299"
          ]
        }
      ],
      "source": [
        "ans = convert_into_output(all_tokens, all_intents, all_seqs, test_file_id, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fBarcgXLXnr",
        "outputId": "3179acdd-cb40-46a1-a747-b55d282914af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'intent': 'Bật thiết bị',\n",
              "  'file': 'qPANF1Bx3XpmuIEjlPUm9Ez.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'bật'},\n",
              "   {'type': 'time at', 'filler': '9 giờ 40 phút'}]},\n",
              " {'intent': 'Giảm độ sáng của thiết bị',\n",
              "  'file': '8LcLj1sHy9xAZF4ibvlPFca.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'giảm'},\n",
              "   {'type': 'changing value', 'filler': '33%'}]},\n",
              " {'intent': 'Tăng độ sáng của thiết bị',\n",
              "  'file': 'Z5G73Vc0YuNWlgV48QZYyQD.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'tăng'},\n",
              "   {'type': 'changing value', 'filler': '88%'}]},\n",
              " {'intent': 'Tắt thiết bị',\n",
              "  'file': 'jTA7bLvVi4zxyXt8c3ePOTT.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'tắt'},\n",
              "   {'type': 'device', 'filler': 'bóng sân'}]},\n",
              " {'intent': 'Đóng thiết bị',\n",
              "  'file': 'QaiOJwzIYKRxVrLDQHxODfn.wav',\n",
              "  'entities': [{'type': 'command', 'filler': 'đóng'},\n",
              "   {'type': 'device', 'filler': 'lò nướng'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "ans[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HvN-RjVNL_T7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"./predictions.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in ans:\n",
        "        json.dump(line, f)\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWsB4vdu8Op6",
        "outputId": "19d7ca91-20bb-4173-ad5b-ae523779a640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "tBMr_GrpEVpH"
      },
      "outputs": [],
      "source": [
        "!cp -r ./checkpoint/slu_intent.pt ./drive/MyDrive/checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovg7oluZPyD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}