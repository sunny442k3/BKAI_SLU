{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8171531-d05b-4b6f-8733-a0a160b49191",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U\n",
    "!pip install transformers soundfile datasets jiwer gdown pyctcdecode kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e75f638d-3b90-4aa5-9bfe-86a7b454e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜./datasetâ€™: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1ZepptsTrVSjQEx-dpBBmQ2b7xYFLn_64\n",
      "From (redirected): https://drive.google.com/uc?id=1ZepptsTrVSjQEx-dpBBmQ2b7xYFLn_64&confirm=t&uuid=69efc103-117f-4b88-8754-1c91eb5131a7\n",
      "To: /workspace/dataset/public_test.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131M/131M [00:01<00:00, 85.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "!mkdir ./dataset\n",
    "import gdown\n",
    "def drive_download(idx, output):\n",
    "    url = 'https://drive.google.com/uc?id=' + idx\n",
    "    gdown.download(url, output, quiet=False)\n",
    "drive_download(\"1ZBL3h6bHMmd8MIUNXqg72PucUkC9ZSWJ\", \"./dataset/train_data.zip\")\n",
    "# drive_download(\"1ZepptsTrVSjQEx-dpBBmQ2b7xYFLn_64\", \"./dataset/public_test.zip\")\n",
    "# drive_download(\"1K_07kix1OgBGO2FNPh-Lxqr1yLbtqFYt\", \"./dataset/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece4476-076d-4c7e-a18e-c2863df6b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ./dataset/public_test.zip -d ./dataset/test\n",
    "!unzip ./dataset/train_data.zip -d ./dataset/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06ae7ae-8e13-4bfa-973b-a6a7c3d6727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, utils, os\n",
    "import numpy as np \n",
    "from functools import partial\n",
    "from datasets import load_metric\n",
    "from dataset import Wav2VecDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6089a0-d312-4335-b172-c4d99509148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(processor, batch):\n",
    "    processed_batch = [\n",
    "        processor(i[\"input_values\"], text=i[\"label\"], sampling_rate=16000) for i in batch\n",
    "    ]\n",
    "    input_features = [{\"input_values\": i.input_values[0]} for i in processed_batch]\n",
    "    input_features = processor.feature_extractor.pad(input_features, padding=True, return_tensors=\"pt\")\n",
    "    if batch[0][\"label\"] is None:\n",
    "        return input_features\n",
    "    label_features = [{\"input_ids\": i.labels} for i in processed_batch]\n",
    "    label_features = processor.tokenizer.pad(\n",
    "        label_features,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    label_features = label_features[\"input_ids\"].masked_fill(label_features.attention_mask.ne(1), -100)\n",
    "    input_features[\"labels\"] = label_features\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0855ea-78a8-4fca-aae4-c4f7daccf844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(root_path, notation_file, test_size=0.3):\n",
    "    notations = utils.load_annotation(notation_file)\n",
    "    dataset = Wav2VecDataset(root_path, [i[\"file\"] for i in notations], notations)\n",
    "    N = len(dataset)\n",
    "    print(f\"Len dataset: {N}\")\n",
    "    train_size = int(N * (1-test_size))\n",
    "    train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, N-train_size])\n",
    "    return train_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238cecc8-2bc7-4100-8551-51a992bfdf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len dataset: 7490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5243, 2247)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds = train_test_split(\"./dataset/train/Train/\", \"./train.jsonl\", test_size=0.3)\n",
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85c6ffe-cb6e-4755-9e02-73abaea0f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a696717c9b2a4230a05cf80ab7e092ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/263 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vn_base_vocab.json\")\n",
    "extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"nguyenvulebinh/wav2vec2-large-vi-vlsp2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf9b7b6-fd9c-42ab-b214-2c1b8b33c0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14026fcd4b11407fa911693533116355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/2.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ded0f7f2a4143f8ad1dc0f707e30b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/wav2vec2-large-vi-vlsp2020 were not used when initializing Wav2Vec2ForCTC: ['feature_transform.bn2.bias', 'feature_transform.bn2.running_var', 'feature_transform.bn3.running_var', 'feature_transform.bn1.running_var', 'feature_transform.bn1.weight', 'feature_transform.bn1.bias', 'feature_transform.bn3.running_mean', 'feature_transform.linear3.weight', 'feature_transform.linear3.bias', 'feature_transform.linear2.weight', 'feature_transform.bn3.weight', 'feature_transform.bn1.running_mean', 'feature_transform.bn2.weight', 'feature_transform.bn3.num_batches_tracked', 'feature_transform.bn3.bias', 'feature_transform.bn2.num_batches_tracked', 'feature_transform.linear2.bias', 'feature_transform.bn2.running_mean', 'feature_transform.linear1.bias', 'feature_transform.bn1.num_batches_tracked', 'feature_transform.linear1.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "processor = Wav2Vec2Processor(tokenizer=tokenizer, feature_extractor=extractor)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"nguyenvulebinh/wav2vec2-large-vi-vlsp2020\",\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.1,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    # vocab_size=len(processor.tokenizer)\n",
    ")\n",
    "model.freeze_feature_encoder()\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.vocab_size = 111\n",
    "# 768 - base model\n",
    "model.lm_head = torch.nn.Linear(in_features=1024, out_features=111, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7967b808-a972-4194-be40-d9afb829db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_400/738329927.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e348a6521a2e4d2095b4231b3b75aace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wer_metric = load_metric(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ba8955-0d04-45cc-990d-86fb5d5c0d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "steps = len(train_ds) // batch_size\n",
    "steps\n",
    "# all_steps = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8a3ba6-0864-4d24-9bee-3e71c094ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec_large_st_1\",\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500//2,\n",
    "    \n",
    "    save_steps=steps,\n",
    "    eval_steps=steps,\n",
    "    logging_steps=steps,\n",
    "    max_steps=steps*15,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c405bb-532b-409f-886f-045cc3eb3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=partial(custom_collate, processor),\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854064fd-dbe2-4528-b943-a673e21a68b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2445' max='2445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2445/2445 35:21, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>13.892500</td>\n",
       "      <td>12.550158</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>4.545400</td>\n",
       "      <td>4.260853</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>3.446000</td>\n",
       "      <td>3.849141</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>3.378500</td>\n",
       "      <td>3.476312</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>3.274700</td>\n",
       "      <td>3.288993</td>\n",
       "      <td>0.999960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>2.852800</td>\n",
       "      <td>1.872195</td>\n",
       "      <td>0.951879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1141</td>\n",
       "      <td>1.305100</td>\n",
       "      <td>0.699217</td>\n",
       "      <td>0.410770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>0.726400</td>\n",
       "      <td>0.456116</td>\n",
       "      <td>0.285528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1467</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>0.358836</td>\n",
       "      <td>0.239422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.309599</td>\n",
       "      <td>0.219865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1793</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.276845</td>\n",
       "      <td>0.203666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1956</td>\n",
       "      <td>0.342100</td>\n",
       "      <td>0.260823</td>\n",
       "      <td>0.191182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2119</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.251177</td>\n",
       "      <td>0.183952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2282</td>\n",
       "      <td>0.310800</td>\n",
       "      <td>0.244030</td>\n",
       "      <td>0.180514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2445</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.240390</td>\n",
       "      <td>0.179013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2445, training_loss=2.4054909610553263, metrics={'train_runtime': 2123.8124, 'train_samples_per_second': 36.839, 'train_steps_per_second': 1.151, 'total_flos': 1.009551511959126e+19, 'train_loss': 2.4054909610553263, 'epoch': 14.91})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e852a-7472-402f-8dbe-de26370654e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainOutput(\n",
    "#     global_step=2445, \n",
    "#     training_loss=2.4054909610553263, \n",
    "#     metrics={\n",
    "#         'train_runtime': 2123.8124, \n",
    "#         'train_samples_per_second': 36.839, \n",
    "#         'train_steps_per_second': 1.151, \n",
    "#         'total_flos': 1.009551511959126e+19, \n",
    "#         'train_loss': 2.4054909610553263, \n",
    "#         'epoch': 14.91\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61edad33-f4b0-4fdf-84c5-b47e08ea5e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./wav2vec_large_st_1/checkpoint-2445/pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "875ef399-8294-4c27-b6d9-91ffa66fb0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7490"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notations = utils.load_annotation(\"./train.jsonl\")\n",
    "dataset = Wav2VecDataset(\"./dataset/train/Train/\", [i[\"file\"] for i in notations], notations)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef2c2505-4344-43d5-933b-dc3f73f7622e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "steps = len(dataset) // batch_size\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ec75980-6570-4e1f-aacf-88e4a53c2e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7020' max='7020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7020/7020 1:10:10, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.169837</td>\n",
       "      <td>0.143994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.138994</td>\n",
       "      <td>0.131735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1404</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.115408</td>\n",
       "      <td>0.123338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>0.218900</td>\n",
       "      <td>0.097054</td>\n",
       "      <td>0.119487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>0.091720</td>\n",
       "      <td>0.116001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2808</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.087326</td>\n",
       "      <td>0.115766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3276</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.073760</td>\n",
       "      <td>0.112256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3744</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.071943</td>\n",
       "      <td>0.113881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4212</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>0.063751</td>\n",
       "      <td>0.107922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.060181</td>\n",
       "      <td>0.106603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5148</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>0.057531</td>\n",
       "      <td>0.108452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5616</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.053092</td>\n",
       "      <td>0.104990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6084</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.053351</td>\n",
       "      <td>0.107640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6552</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.106144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>0.106827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7020, training_loss=0.17307361211532202, metrics={'train_runtime': 4211.8219, 'train_samples_per_second': 26.668, 'train_steps_per_second': 1.667, 'total_flos': 1.4168328886976856e+19, 'train_loss': 0.17307361211532202, 'epoch': 14.97})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec_large_st_2\",\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size*2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=1000,\n",
    "    \n",
    "    save_steps=steps,\n",
    "    eval_steps=steps,\n",
    "    logging_steps=steps,\n",
    "    max_steps=steps*15,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=partial(custom_collate, processor),\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "901211ce-77de-4095-8e65-b4abc1185ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./wav2vec_large_st_2/checkpoint-5616/pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f097f497-e933-4f6d-b907-d3255b999057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1299"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "test_ds = Wav2VecDataset(\"./dataset/test/public_test\", os.listdir(\"./dataset/test/public_test\"))\n",
    "len(test_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ece651d-3f53-48a5-915a-79e24a642043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_collate(processor, batch):\n",
    "    processed_batch = [\n",
    "        processor(i[\"input_values\"], sampling_rate=i[\"sample_rate\"]) for i in batch\n",
    "    ]\n",
    "    input_features = [{\"input_values\": i.input_values[0]} for i in processed_batch]\n",
    "    input_features = processor.pad(input_features, padding=True, return_tensors=\"pt\")\n",
    "    input_features[\"id\"] = [i[\"file\"] for i in batch]\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35428bb1-0954-444a-87ae-e59da989ba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_ds, shuffle=False, batch_size=32, collate_fn=partial(test_collate, processor))\n",
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "087c03e6-b161-4e26-a43f-863f9fcecc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2vec_inference(model, test_loader, processor, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval() \n",
    "    model = model.to(device)\n",
    "    pred_sentences = {}\n",
    "    for idx, batch in enumerate(test_loader, 1):\n",
    "        X_test =  batch[\"input_values\"].to(device)\n",
    "        X_test = X_test.half()\n",
    "        file_test = batch[\"id\"]\n",
    "        with torch.set_grad_enabled(False):\n",
    "            logits = model(input_values=X_test).logits\n",
    "        logits = torch.argmax(logits, dim=-1)\n",
    "        transcriptions = processor.batch_decode(logits, skip_special_tokens=True)\n",
    "        for file_id, trans in zip(file_test, transcriptions):\n",
    "            pred_sentences[file_id] = trans\n",
    "        # pred_sentences += transcriptions\n",
    "        print(\"\\r\", end=\"\")\n",
    "        print(f\"\\r {idx} / {len(test_loader)}\", end = \"\" if idx != len(test_loader) else \"\\n\")\n",
    "    return pred_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3ff7ceb-22ef-4745-b90d-34cc1829e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41 / 41\n"
     ]
    }
   ],
   "source": [
    "pred_sens = wav2vec_inference(model, test_loader, processor, torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05e9f4d6-18a2-491d-ad22-6a1e7083daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./w2v_lasted_test_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pred_sens, f, ensure_ascii=False)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37722153-5076-4ee3-8a3d-35714df840d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
